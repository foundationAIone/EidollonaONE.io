name: ci
on:
  push:
  pull_request:
jobs:
  build-test-audit:
    runs-on: windows-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.9'
          cache: 'pip'
      - name: Install deps
        run: |
          python -m pip install -U pip wheel
          if (Test-Path requirements.txt) { pip install -r requirements.txt }
          if (Test-Path requirements-dev.txt) { pip install -r requirements-dev.txt }
          pip install pytest pip-audit pytest-cov
      - name: Editable install
        run: pip install -e .
      - name: Hygiene (Analyze)
        shell: pwsh
        run: pwsh -NoProfile -File .\scripts\repo_hygiene.ps1 -Analyze
      - name: SE41 enforce
        run: python .\tools\enforce_symbolic_v41.py
      - name: Tests (fast)
        run: pytest -q
      - name: Test (cov)
        run: pytest --cov=. --cov-report=xml --cov-report=term --cov-fail-under=60
      - name: Upload coverage.xml
        uses: actions/upload-artifact@v4
        with:
          name: coverage
          path: coverage.xml
      - name: Compare perf baseline
        shell: pwsh
        run: |
          python scripts/perf_smoke.py > perf_current.json
          python - <<'PY'
          import json, sys, pathlib
          cur = json.loads(pathlib.Path('perf_current.json').read_text())
          base = json.loads(pathlib.Path('scripts/perf_baseline.json').read_text())
          fail = []
          if isinstance(cur.get('encryption_chacha_ops_per_sec'), (int, float)) and isinstance(base.get('encryption_chacha_ops_per_sec'), (int, float)):
              if cur['encryption_chacha_ops_per_sec'] < 0.8 * base['encryption_chacha_ops_per_sec']:
                  fail.append('Perf regression: ChaCha ops/sec')
          if isinstance(cur.get('symbolic_eval_latency_ms'), (int, float)) and isinstance(base.get('symbolic_eval_latency_ms'), (int, float)):
              if cur['symbolic_eval_latency_ms'] > 1.5 * base['symbolic_eval_latency_ms']:
                  fail.append('Perf regression: SE41 eval latency')
          if fail:
              print('\n'.join(fail))
              sys.exit(1)
          PY
      - name: Upload perf snapshot
        uses: actions/upload-artifact@v4
        with:
          name: perf-current
          path: perf_current.json
      - name: pip-audit (json)
        shell: pwsh
        run: |
          if (Test-Path requirements.txt) {
            pip-audit -r requirements.txt -f json -o pip-audit.json || $LASTEXITCODE=0
          } else {
            pip-audit -f json -o pip-audit.json || $LASTEXITCODE=0
          }
      - name: Upload audit
        uses: actions/upload-artifact@v4
        with:
          name: pip-audit
          path: pip-audit.json
